#!/bin/bash
######################################################
#__author__ = "Xin Wang"
#__email__ = "wangxin@nii.ac.jp"
#
# Demonstration script
#
# Usage:
#   $: bash 00_demo.sh MODEL/RUN
#   where MODEL is the name of the model, and RUN is the 
#   index of the training round.
#
# For example:
#   $: bash 00_demo.sh lfcc-lcnn-lstmsum-p2s/01
#
# You may also run this script in background
#   $: bash 00_demo.sh lfcc-lcnn-lstmsum-p2s/01 > log_batch 2>$1 &
#   Then you can quit the terminal ($: exit) and let the job run
#   You will find the results in file log_batch

# This script will
#  0. download pre-trained models and pre-generated scores
#     by Xin (if they are not available)
#  1. compute EER on scores generated by Xin 
#     (using the same pre-trained model on NII's server)
#  2. run pre-trained model on ASVspoof2019 LA evalset,
#     calculate EER
#  3. train a new model, run evaluation, and compute EER
#
# Step3 requires 16GB GPU memory for model training
# If it is too much for your GPU, please reduce --batch-size 
# in */*/00_train.sh before running this script
######################################################

RED='\033[0;32m'
NC='\033[0m'

##############
# Configurations
#  environment config file in github repo
#  You may need to load conda environment in this env.sh
ENVFILE=$PWD/../../env.sh

# a wrapper to run EER and min-tDCF, given scores by the model
EVALSCRIPT=$PWD/02_evaluate.py

# script of main.py (used by all the models)
MAINSCRIPT=$PWD/01_main.py

# configuration to run the model (shared by all the models)
CONFIGSCRIPT=$PWD/01_config.py

# for convenience, trial length are logged into these binary files
#  They will be automatically generated if not available. 
#  Make them available save the time to scan files
CONVDIR=$PWD/conv

# download link for pre-trained models
#  don't change these two
MODELLINK=https://www.dropbox.com/sh/bua2vks8clnl2ha/AAA8WFhdk33PpgeY0rSQt0iTa/project-03-asvspoof-mega-pretrained.tar
MODELNAME=project-03-asvspoof-mega-pretrained
MD5SUMVAL=e580a9a563543acafb360205e34c6b34
########

#############
# step 0. download files if necessary
if [[ -e "./${MODELNAME}.tar" ]];then
    TMP=`md5sum ${MODELNAME}.tar | awk '{print $1}'`
    if [ ${TMP} != ${MD5SUMVAL} ]; then
	rm ${MODELNAME}.tar
	echo -e "Re-download ${MODELNAME}.tar"
    else
	echo -e "Found ${MODELNAME}.tar"
    fi
fi

if [[ ! -e "./${MODELNAME}.tar" ]];then
    echo -e "${RED}Downloading pre-trained model${NC}"
    wget -q ${MODELLINK}
fi

if [ -e "./${MODELNAME}.tar" ];then	
    echo -e "${RED}Untar pre-trained models${NC}"
    tar -xzf ${MODELNAME}.tar
else
    echo "Cannot download ${MODELLINK}. Please contact the author"
    exit
fi
#############
# setup PYTHONPATH and conda
source ${ENVFILE}

# go to the model directory
MODEL=$1

if [ -z $MODEL ];
then
    echo -e "\n${RED}Please specify model directory, for example: ${NC}"
    echo -e "$: bash 00_demo.sh lfcc-lcnn-lstmsum-p2s/01 \n"
    exit
else
    echo -e "\n${RED}Use model ${MODEL}${NC}"
    cd ${MODEL}
fi

#############
# step 1. EER on scores produced by Xin
echo -e "\n${RED}=======================================================${NC}"
echo -e "${RED}Step1. computing EER on pre-produced scores${NC}"
echo -e "(Scores were produced by pre-trained ${MODEL} on NII's server)" 
LOGFILE=__pretrained/log_output_testset
python ${EVALSCRIPT} ${LOGFILE}

#############
# step 2. run pre-trained model by Xin and compute EER
echo -e "\n${RED}=======================================================${NC}"
echo -e "${RED}Step2. run pre-trained ${MODEL} on eval set using your GPU server${NC}"
echo -e "The job will run in background for ~20 minutes. Please wait."
echo -e "(Model ${MODEL} was trained on NII's server.)"


cp ${MAINSCRIPT} ./main.py
cp ${CONFIGSCRIPT} ./config.py

LOGFILE=log_output_testset_pretrained
python main.py --inference --model-forward-with-file-name --trained-model __pretrained/trained_network.pt > ${LOGFILE} 2>&1

echo -e "\n${RED}This is the result using pre-trained model on your GPU ${NC}"
python ${EVALSCRIPT} ${LOGFILE}

#############
# step 3. train new model, and run evaluation
echo -e "\n${RED}=======================================================${NC}"
echo -e "${RED}Step3. train a new model ${MODEL} using your GPU server${NC}"
echo -e "Current training recipe requires 16GB GPU memory."
echo -e "If it is too much for your GPU, please reduce --batch-size in */*/00_train.sh before running this step"
echo -e "The job will run in backgroun for a few hours. Please wait."
echo -e "You can also run this script in background. See README of this script"

# Copy cached files that logs utterance duration. This saves time
# It is also OK to skip this step, and the code will generate them
cp ${CONVDIR}/* ./

# train using prepared script 
# (notice that random seed is different from different RUN)
bash 00_train.sh

echo -e "\n${RED}Evaluating the trained model ${NC}"
echo -e "The job will run in backgroun for ~20 minutes. Please wait."
LOGFILE=log_output_testset
python main.py --inference --model-forward-with-file-name > ${LOGFILE} 2>&1

echo -e "\n${RED}This is the result produced by your trained model  ${NC}"
python ${EVALSCRIPT} ${LOGFILE}

echo -e "\nThe result may be different from pre-trained models due to GPU type, Pytorch ver, and so on"

exit 


